{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Antti Kiviaho\n",
    "# Date 24.2.2023\n",
    "#\n",
    "# This script is for merging clusters from separate Visium experiments/samples.\n",
    "# The clusters are derived from a combination of GE and spatial proximity graph\n",
    "# The shared latent space is derived from SCANORAMA integration.\n",
    "# Now we merge clusters in this space using MNN and silhouette scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging sample specific clusters to find joint clusters in the scanorama space.\n",
    "The criteria here is that a cluster is merged with it's closest (Euclidean distance in the 100 dim space)\n",
    "cluster if \n",
    "1) this improves the mean silhouette score of both clusters.\n",
    "2) the nearest cluster originates from a different sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "import seaborn as sns\n",
    "from  scripts.utils import load_from_pickle,save_to_pickle, get_sample_ids\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "adata_scanorama = load_from_pickle('./visium_after_scanorama_clustered.pickle')\n",
    "normalized_adata = load_from_pickle('./data/clustered_visium_data.pickle')\n",
    "samples = get_sample_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_concat = sc.concat(normalized_adata)\n",
    "if (adata_concat.obs_names == adata_scanorama.obs_names).all():\n",
    "    adata_scanorama.obs['spatial_cluster'] = adata_concat.obs['sample_id']+'_'+adata_concat.obs['joint_leiden_clusters']\n",
    "del adata_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "## Calclulates the euclidean distance on cluster level yield to information about which clusters are closest to each other\n",
    "def calculate_distances(cluster_means_df,n_closest=5):\n",
    "    closest_names = np.empty((0,n_closest))\n",
    "    closest_dist = np.empty((0,n_closest))\n",
    "\n",
    "    for idx in range (len(cluster_means_df)):\n",
    "\n",
    "        # Divide the df into the query (1 cluster) and the rest\n",
    "        comparison_dat = cluster_means_df.iloc[idx]\n",
    "        rest_of_the_dat = cluster_means_df.drop(index=[cluster_means_df.index[idx]])\n",
    "\n",
    "        # Repeat one row to match the shape of the rest\n",
    "        comparison_dat_reshaped = np.tile(comparison_dat,(rest_of_the_dat.shape[0],1))\n",
    "        res = np.linalg.norm(rest_of_the_dat-comparison_dat_reshaped,axis=1)\n",
    "        matches = np.asarray(list(rest_of_the_dat.index[np.argsort(res)[:n_closest]]))\n",
    "\n",
    "        # Stack names and distances of closest matches\n",
    "        closest_names = np.vstack([closest_names,matches])\n",
    "        closest_dist = np.vstack([closest_dist,np.sort(res)[:n_closest]])\n",
    "\n",
    "    # Convert to dataframes for easy slicing\n",
    "    closest_names = pd.DataFrame(closest_names,index=cluster_means_df.index)\n",
    "    closest_dist = pd.DataFrame(closest_dist,index=cluster_means_df.index)\n",
    "    return(closest_names,closest_dist)\n",
    "\n",
    "# Compare sample names of the index and the first column to find if the nearest cluster is from another sample. Only keep those\n",
    "def find_nearest_neighbors(df):\n",
    "    slices = []\n",
    "    for idx in range(len(df)):\n",
    "        if (('_').join(df.index[idx].split('_')[:-1]) != \n",
    "        ('_').join(df[0][idx].split('_')[:-1])):\n",
    "            slices.append(idx)\n",
    "\n",
    "    # These are the spatial clusters with different sample nearest cluster\n",
    "    return(df.iloc[slices])\n",
    "\n",
    "# Keep clusters that are mutual nearest neighbors (MNN)\n",
    "def find_mnn_clusters(df):\n",
    "    slices = []\n",
    "    for idx in range(len(df)):\n",
    "        if (df[0][idx] in df.index):\n",
    "            if df.loc[df[0][idx]][0] == df.index[idx]:\n",
    "                if df[0][idx] not in df.iloc[slices].index:\n",
    "                    slices.append(idx)\n",
    "\n",
    "    # These are the spatial clusters with mutual nearest neighbors\n",
    "    return(df.iloc[slices])\n",
    "\n",
    "\n",
    "def has_no_duplicates(cluster_list):\n",
    "    '''\n",
    "    Check if a sample is duplicated in a cluster\n",
    "    '''\n",
    "    return(len([('_').join(s.split('_')[:-1]) for s in cluster_list]) == len(\n",
    "        set([('_').join(s.split('_')[:-1]) for s in cluster_list])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts spot level scanorama coordinates and averages to cluster level for mutual nearest neighbor comparison\n",
    "n_comps = 100 # 100\n",
    "spot_lvl_data = pd.DataFrame(adata_scanorama.obsm['X_scanorama'],index=adata_scanorama.obs_names).iloc[:,:n_comps]\n",
    "spot_lvl_data['spatial_cluster'] = adata_scanorama.obs['spatial_cluster']\n",
    "spot_lvl_labs = adata_scanorama.obs['spatial_cluster']\n",
    "\n",
    "# Speeds up the analysis a little bit\n",
    "distance_array = pairwise_distances(adata_scanorama.obsm['X_scanorama'][:,:n_comps])\n",
    "ref_score = silhouette_score(distance_array,spot_lvl_labs,metric='precomputed')\n",
    "ref_score # = -1 # If the reference score is set to -1, we effectively lose the silhouette score if-clause\n",
    "\n",
    "# A scanorama-based clustering silhouette score is 0.06169539683589392 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_score_ok(new_labs,df=distance_array, reference = ref_score):\n",
    "    '''\n",
    "    Checks the silhouette score of new clustering labels and compares them to a reference of \n",
    "    the original clustering\n",
    "    '''\n",
    "    new_score = silhouette_score(df,new_labs,metric='precomputed')\n",
    "    if new_score > reference:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "it = 1\n",
    "prev_iteration = 0\n",
    "while True:\n",
    "    if 'modified_labs' in locals():\n",
    "        # Change the labels by which means are calculated\n",
    "        spot_lvl_data['spatial_cluster'] = modified_labs\n",
    "    else:\n",
    "        modified_clusters = dict()\n",
    "\n",
    "    # Calculate cluster means in the scanorama space\n",
    "    cluster_lvl_dat = spot_lvl_data.groupby('spatial_cluster').mean()\n",
    "\n",
    "    # Calculate euclidean distance between the clusters\n",
    "    names, dist = calculate_distances(cluster_lvl_dat)\n",
    "\n",
    "    # Find MNN clusters that are from different samples\n",
    "    filtered_names = find_nearest_neighbors(names)\n",
    "    cluster_pairs = find_mnn_clusters(filtered_names)\n",
    "    \n",
    "    # The break condition: If there are no more eligible clusters to merge.\n",
    "    if len(cluster_pairs) == prev_iteration:                                              # REPLACE THIS WITH ANOTHER CONDITION, SINCE SOME CLUSTERS LINGER\n",
    "        print('Ran for '+ str(it)+ ' iterations.')\n",
    "        print('Merged clusters:')\n",
    "        print(modified_clusters)\n",
    "        break\n",
    "\n",
    "    prev_iteration = len(cluster_pairs)\n",
    "    n = 1\n",
    "    # Find mergeable clusters by checking different conditions\n",
    "    # Iterate over the clusters\n",
    "    for idx in range(len(cluster_pairs)):\n",
    "        inst_1 = str(cluster_pairs.index[idx])\n",
    "        inst_2 = str(cluster_pairs.iloc[idx][0])\n",
    "        message = (inst_1 + ' <--> ' + inst_2 + ' merge improves the score!')\n",
    "\n",
    "        # Check if the index is already a merged cluster\n",
    "        if inst_1 in modified_clusters.keys():\n",
    "            prev_lst = modified_clusters[inst_1]\n",
    "            replaced = prev_lst + [inst_2]\n",
    "            if is_score_ok(spot_lvl_labs.replace(replaced,'test_cluster')) & has_no_duplicates(replaced):\n",
    "                print(message)\n",
    "                print()\n",
    "                modified_clusters[inst_1] = replaced\n",
    "\n",
    "        # Check if the first column is already a merged cluster\n",
    "        elif inst_2 in modified_clusters.keys():\n",
    "            prev_lst = modified_clusters[inst_2]\n",
    "            replaced = prev_lst + [inst_1]\n",
    "            if is_score_ok(spot_lvl_labs.replace(replaced,'test_cluster')) & has_no_duplicates(replaced):\n",
    "                print(message)\n",
    "                print()\n",
    "                modified_clusters[inst_2] = replaced\n",
    "        \n",
    "        # If neither is a modified cluster, create a new cluster and save it\n",
    "        else:\n",
    "            replaced = [inst_1,inst_2]\n",
    "            if is_score_ok(spot_lvl_labs.replace(replaced,'test_cluster')) & has_no_duplicates(replaced):\n",
    "                print(message)\n",
    "                print()\n",
    "                modified_clusters['new_clust_'+str(n)] = replaced\n",
    "                n+=1\n",
    "\n",
    "    modified_labs = spot_lvl_labs.copy()\n",
    "    for k in modified_clusters.keys():\n",
    "        modified_labs = modified_labs.replace(modified_clusters[k],k)\n",
    "\n",
    "    print('Iteration '+ str(it) + ' completed.')\n",
    "    print()\n",
    "    it+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data object with the labels to a pickle file.\n",
    "adata_scanorama.obs['merged_spatial_clusters'] = modified_labs\n",
    "adata_scanorama.uns['merged_spatial_clusters_dict'] = modified_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the merged clustering to individual spatial sections\n",
    "identifier = 'merged_spatial_clusters' # spatial_cluster, sample_id, clusters\n",
    "for s in samples:\n",
    "    if (normalized_adata[s].obs_names ==\n",
    "    adata_scanorama[adata_scanorama.obs['sample_id']==s].obs_names).all():\n",
    "        normalized_adata[s].obs[identifier] = adata_scanorama[adata_scanorama.obs['sample_id']==s].obs[identifier].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data objects...\n",
    "#save_to_pickle(adata_scanorama,'./data/visium_after_scanorama_with_all_embeddings.pickle')\n",
    "#save_to_pickle(normalized_adata,'./data/individual_sections_normalized_clustered.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the clusters on UMAP and on sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = 'merged_spatial_clusters' # spatial_cluster, sample_id, clusters\n",
    "targets = adata_scanorama.obs[adata_scanorama.obs[identifier].isin(modified_clusters)][identifier].unique()\n",
    "targets = targets.remove_unused_categories()\n",
    "\n",
    "adata_scanorama.obs['visualized_clusters'] = adata_scanorama.obs[identifier][adata_scanorama.obs[identifier].isin(targets)]\n",
    "adata_scanorama.obs['visualized_clusters'] = adata_scanorama.obs['visualized_clusters'].cat.set_categories(targets)\n",
    "\n",
    "fig,ax1 = plt.subplots(1,1)\n",
    "fig.set_size_inches(6,6)\n",
    "fig.set_dpi(120)\n",
    "sc.pl.umap(adata_scanorama, color=['visualized_clusters'],ax=ax1,size=8,frameon=True,palette='tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = 'merged_spatial_clusters' # joint_leiden_clusters, sample_id\n",
    "img_dir_path = './plots/merged_spatial_clusters_on_sections'\n",
    "Path(img_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "for sample in samples:\n",
    "    targets = adata_scanorama.obs[identifier].unique()\n",
    "    targets = [t for t in targets if 'new_clust' in t]\n",
    "\n",
    "    adata_vis = normalized_adata[sample].copy()\n",
    "    adata_vis.obs['visualized_clusters'] = adata_vis.obs[identifier][adata_vis.obs[identifier].isin(targets)].cat.set_categories(targets)\n",
    "\n",
    "    fig,ax1 = plt.subplots(1,1)\n",
    "    fig.set_size_inches(12,12)\n",
    "    fig.set_dpi(150)\n",
    "    sq.pl.spatial_scatter(adata_vis,color=['visualized_clusters'],ax=ax1,size=1.2,frameon=False,palette='tab10')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    fig.savefig(img_dir_path+'/'+sample+'.png')\n",
    "    fig.clf()\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
